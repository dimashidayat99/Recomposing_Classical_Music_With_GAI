# Recomposing Classical Music with Generative AI

![](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/6072d90a-82d6-4aae-80af-deca88fab090)


# Background of the Study

Music is a universal language that transcends cultural boundaries and speaks to the depths of human emotion and experience. Classical music is one of the most enduring and beloved forms of music. Rooted in centuries of tradition and innovation, classical music encompasses a vast repertoire of compositions that have stood the test of time.  Classical music, with its rich history and diverse repertoire, has long been a source of inspiration and admiration for musicians and audiences alike. For its complexity, beauty, and emotional depth, the compositions of great classical composers such as Bach, Beethoven, Mozart, and others are revered.

Even though classical music is not really popular today, classical music does not remain static. It's continuing to evolve and be an inspiration for new generations of musicians and composers. The use of technology, particularly artificial intelligence (AI), is one way in which classical music has evolved. AI can change the way music is composed, performed, and listened to, opening up new possibilities for creativity and expression. The growing interest in using AI in recent years has promoted the usage of AI in so many fields including music generation. Generative AI, in particular, has shown promise in this regard. By analyzing vast amounts of musical data, generative AI algorithms can learn the patterns and structures that define a particular style or genre of music.  This allows them to compose new compositions that are stylistically similar to those of the composers they've been trained in. 

There are a number of possible benefits to the revival of classic music using generative AI. New instruments and techniques for the creation of music may be made available to composers, enabling them to explore new creative possibilities. Introducing classical music to new audiences in a fresh and engaging way can also help preserve and support classical music. The use of generative artificial intelligence for music composition also presents challenges and considerations that need to be addressed. Ensuring that the compositions created with artificial intelligence are original and creative rather than merely imitating existing works is one of the main challenges. In addition, questions are being raised about the role of a composer in AI's creative process and how to attribute and recognize music generated by artificial intelligence.

# Research Questions

The project will based on the following research questions. These research questions are designed to provide a comprehensive exploration of the use of generative AI for recomposing classical music, covering technical, and artistic aspects of this emerging field.

1. How to create generative AI algorithms to produce music similar to classical music?
2. What kind of generative AI algorithms can produce the most pleasant music and approach the characteristics of classical music?
3. How do AI-generated classical music compositions compare to established works in the classical repertoire in terms of technical execution and artistic expression, as evaluated by musicians and audiences?

# Research Objectives

The objective of this project is to investigate the use of generative AI for recomposing classical music. Specifically, the study aims to achieve the following objectives:

1. To develop generative AI algorithms to produce music similar to classical music.
2. To evaluate which generative AI algorithms can produce the most pleasant music and approach the characteristics of classical music.
3. To compare the AI-generated classical music compositions with the established work in the classical repertoire in terms of technical execution and artistic expression which evaluated by musicians and audiences.

# Methodology

## Dataset
The dataset used in this project was obtained from [kaggle](https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi). Where the dataset consists of classical piano midi files containing compositions of 19 famous composers scraped from [piano-midi website](http://www.piano-midi.de). From all famous composer, this porject utilizing classical music from only one composer which known as Chopin. The reason why only one composer was used due to the resource limitation, cost and time efficiency.

## EDA 
<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/framework/eda_framework.png"  width="300" />
</p>
<p align="middle">
    <em>Exploratory Data Analysis Workflow.</em>
</p>

The process of Exploratory Data Analysis (EDA) is shown in the figure above where it begins with gathering the classical music midi data. The data underwent a sampling process where only Chopin's music will be chosen. There are two types of EDA, the first type of EDA is directly toward the exploration of the music composition itself without data preprocessing where the music sheet and audio are produced to get a better understanding of the music on how music sounds and how the music notes and durations looks like.

## Music Composition
<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/music_sheet_audio/eda_sheet_music.png"  width="800" />
</p>
<p align="middle">
    <em>Music Sheet of One of the Chopin Composed Music.</em>
</p>


<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/e531ea2b-ec2f-4fc1-b451-e12e0e71d873" width=400 />
</div>
<p align="middle">
    <em>Music Audio of One of the Chopin Composed Music</em>
</p>

The process continued with the data preprocessing where each of the Chopin-composed music underwent clef splitting where the treble and bass clef were separated. Each of the clefs was undergone the music element extraction where the notes, chords, and durations were extracted for the data frame construction. There are two total data frames were created which are the treble data frame and bass data frame, each representing the treble clef and bass clef respectively. Each data frame consists of two features where the first feature is the notes and chords while the second feature is durations. The chords and notes were extracted based on their pitch frequency while the durations were extracted based on the durations type. Each of the data frames underwent the second type of EDA which is more toward data analytics. The EDA includes the univariate analysis, multivariate analysis and outliers analysis. 

## Univariate Analysis
### Bass Clef
#### Pitch Frequency Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/pitch_frequency_distribution_bass.png"  width="800" />
</p>
<p align="middle">
    <em>Pitch Frequency Distribution for Bass Clef Data.</em>
</p>

The distribution of pitch frequency is positively skewed with the modes around 160 $s^-{1}$. Majority of the chords and notes used is in the frequencies of 50 $s^{-1}$ to 300 $s^{-1}$. While chords and notes with frequencies around 500 $s^{-1}$ and above can be seen as minority chords and notes used in the Chopin-composed music. 

#### Duration Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/durations_distribution_bass.png"  width="800" />
</p>
<p align="middle">
    <em>Duration Distribution for Bass Clef Data.</em>
</p>

Majority of the durations used are 8th notes with around 17500 counts followed by 16th note with around 14000 counts. The quarter notes is in the third position with around 7500 count. Other than the mentioned durations which are whole note, half note, complex note, breve note and 32nd note are the minority durations in the Chopin-compoased music.

### Treble Clef
#### Pitch Frequency Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/pitch_frequency_distribution_treble.png"  width="800" />
</p>
<p align="middle">
    <em>Pitch Frequency Distribution for Treble Clef Data.</em>
</p>

The distribution of pitch frequency is positively skewed with the modes around 350 $s^-{1}$. Majority of the chords and notes used is in the frequencies of 200 $s^{-1}$ to 1300 $s^{-1}$. While chords and notes with frequencies around 1500 $s^{-1}$ and above can be seen as minority chords and notes used in the Chopin-composed music.

#### Duration Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/durations_distribution_treble.png"  width="800" />
</p>
<p align="middle">
    <em>Duration Distribution for Treble Clef Data.</em>
</p>

Majority of the duration used is 16th notes followed by 8th note. The quarter notes is in the third position with around 6500 counts. Other than the mentioned durations which are whole note, half note, complex note, breve note and 32nd note are the minority durations in the Chopin compoased music.


## Multivariate Analysis
### Bass Clef

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/multivariate_analysis/durations_vs_pitch_frequency_bass.png"  width="800" />
</p>
<p align="middle">
    <em>Violnin Plot of Duration against Pitch Frequency for Bass Clef Data.</em>
</p>


The 8th note, 16th note and quarter note have almost similar shape of distribution which is almost symmetric. Majority of the notes are in the range of 30 $s^{-1}$ to 400 $s^{-1}$. The range of 8th note distribution is around 20 $s^{-1}$ to 1300 $s^{-1}$ while te range of 16th note distribution is around 20 $s^{-1}$ to 1400 $s^{-1}$. Half note has similar distribution of majority of the notes but postively skewed with the mode around 100 $s^{-1}$. The whole note has a mode around 80 $s^{-1}$ with majority of the notes is in the range of 20 $s^{-1}$ to 200 $s^{-1}$. The complex note has the range of distribution of 0 $s^{-1}$ to 1400 $s^{-1}$ with majority of the notes is in the range of 0 $s^{-1}$ to 400 $s^{-1}$. Both breve note and 32nd note has almost the same shape of distribution with almost same modes around 100 $s^{-1}$. Also, both have the same range of distribtion which from 0 $s^{-1}$ to around 360 $s^{-1}$.

### Treble Clef

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/multivariate_analysis/durations_vs_pitch_frequency_treble.png"  width="800" />
</p>
<p align="middle">
    <em>Violnin Plot of Duration against Pitch Frequency for Treble Clef Data.</em>
</p>

The half note, quarter note, 8th note and 16th note have almost similar shape of distribution which is slight positively skewed with almost similar range where most notes are in the range of 200 $s^{-1}$ to 1000 $s^{-1}$ with the distribution range of 200 $s^{-1}$ to 2900 $s^{-1}$. The whole note has a mode around 450 $s^{-1}$ with distribution range of 150 $s^{-1}$ to 1000 $s^{-1}$. The complex note has the longest range of distribution of 0 $s^{-1}$ to 3000 $s^{-1}$. The breve note distribution is the only negatively skewed in the treble clef dsitribution, the breve note distribution has mode around 500 $s^{-1}$ with the range of 200 $s^{-1}$ to 800 $s^{-1}$. The 32nd note distribution has mode around 500 $s^{-1}$ with the range of 0 $s^{-1}$ to 1750 $s^{-1}$ with the majority of the notes lies in the range of 200 $s^{-1}$ to 800 $s^{-1}$. 

The additional and details of the EDA process can be found in `eda` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/eda)


## Modelling

There are so many generative models available which can be used in this project. However, among all the generaitve models, only 4 models will be used which are autoregressive model or known as Recurrent Neural Network (RNN) by using Long Short-Term Memory (LSTM) architecture, Variational AutoEncoder (VAE), Generative Adversarial Network (GAN) and large language model of Generative Pretrained Transformer (GPT). Another model used in this project is a fusion model which a combination of three different generative model architecture. The fusion model utilizing the RNN of Gate Recurrent Unit (GRU) architecure combine with VAE architecture together with GAN architecture. Each of the model require different representation of the data, therefore, different preprocesing was applied to the data for each model. 

### Autoregressive Model - LSTM

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/lstm/framework/LSTM_framework.png" />
</p>
<p align="middle">
    <em>LSTM Model Framework.</em>
</p>

LSTM is a type of neural network designed to handle sequence data by maintaining long-term dependencies. It does this by using a memory cell that can store information, along with gates that control the flow of information into and out of the cell. This allows LSTM networks to learn patterns in sequences and make predictions based on them, making them useful for the main task in this project which is classical music recomposition. A detail explanation of the LSTM framework as well as the source code can be found in `model\lstm` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/lstm).

### AutoEncoder Model - VAE

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/vae/framework/VAE_framework.png" />
</p>
<p align="middle">
    <em>VAE Model Framework.</em>
</p>


VAE is a type of neural network used for generative modeling. It learns to encode input data into a lower-dimensional latent space and then decode it back to the original input. VAEs are trained to maximize the likelihood of the observed data while also maximizing the mutual information between the latent variables and the data. This allows them to generate new data samples that resemble the training data, making them useful for the main task in this project which is classical music recomposition.  A detail explanation of the VAE framework as well as the source code can be found in `model\vae` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/vae).

### Generative Adversarial Network - GAN

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/gan/framework/GAN_framework.png" />
</p>
<p align="middle">
    <em>GAN Model Framework.</em>
</p>


GAN is a type of neural network used for generative modeling. It consists of two networks: a generator and a discriminator. The generator creates new data samples, while the discriminator tries to distinguish between real and generated samples. They are trained together in an adversarial manner, where the generator tries to fool the discriminator and the discriminator tries to become better at distinguishing real from fake. This competition leads to the generator learning to create increasingly realistic samples, making GANs useful for the main task in this project which is classical music recomposition. A detail explanation of the GAN framework as well as the source code can be found in `model\gan` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/gan).

### Fusion Model - RNN-VAE-GAN

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/fusion/framework/FUS_framework.png" />
</p>
<p align="middle">
    <em>Fusion Model Framework.</em>
</p>

A fusion generative model that combines GRU, VAE, and GAN elements would be a complex architecture aimed at capturing the strengths of each component. The GRU component will be used for sequence modeling. GRUs are a type of RNN that can capture long-range dependencies in sequences, making them suitable for tasks where the context of previous elements is important. While the VAE component would handle the latent space modeling. It would encode input data into a latent space representation, which can then be decoded back into the original data. VAEs are useful for learning meaningful representations of data and generating new samples that resemble the training data. Finally, the GAN component would add a competitive training mechanism to improve the realism of the generated samples. The generator part of the GAN would likely be integrated with the VAE decoder, generating samples from the learned latent space. The discriminator would then provide feedback to both the generator and the VAE on the realism of the generated samples.

In this fusion model, the GRU would help capture the sequential dependencies in the data, the VAE would learn a meaningful latent space representation, and the GAN would improve the quality of the generated samples. By combining these components, the model could potentially achieve better performance than using any one component alone. A detail explanation of the fusion model framework as well as the source code can be found in `model\fusion` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/fusion).

### Large Language Model - GPT

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/gpt/framework/GPT_framework.png" />
</p>
<p align="middle">
    <em>GPT Model Framework.</em>
</p>

GPT is a type of deep learning model based on the transformer architecture. It is designed for natural language processing tasks such as text generation, translation, and question answering. However, since text generation is a sequence data and the music also is a sequence data. The GPT should be able to model music data, in a way that captures the structure and patterns of music. GPT is "pre-trained" on a large corpus of musical data, which allows it to learn the nuances of music elements and context. This pre-training is followed by fine-tuning specific tasks to further improve performance. A detail explanation of the GPT framework as well as the source code can be found in `model\gpt` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/gpt).

# Result & Evaluation

There are three pieces of music generated for each model. The first music was generated by using the key of C major and a tempo of 90 bpm. The second music was generated by using the key of A minor and a tempo of 60 bpm. Finally, the final music was generated by using the key of E minor and a tempo of 120 bpm. All the music generated uses a time signature of 4/4 which is the most common time signature. All the pieces of musics was evaluated by the audience and subject matter expert (musician).

## Results
### LSTM
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/cdc40b47-6368-4b38-bd2b-00e24c1be49c" width=400 />
</div>
<p align="middle">
    <em>Music Generated by LSTM (Key: C Major, Tempo: 90 Bpm)</em>
</p>


#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/109e6fa7-cc9b-40cf-9c01-592e70b09a2f" width=400 />
</div>
<p align="middle">
    <em>Music Generated by LSTM (Key: A Minor, Tempo: 60 Bpm)</em>
</p>


#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/4aab4dd9-0dce-488b-8631-68bf5486cdb3" width=400 />
</div>
<p align="middle">
    <em>Music Generated by LSTM (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on LSTM

Based on the three generated music by LSTM. LSTM has shown some degree of ability to produce pleasant music. This can be proven from the two pieces of generated music (key C major and tempo 90 Bpm) and (key A minor and tempo 60 Bpm) where the both successfuly create a good melody and well structed in the treble clef. However, the bass clef generated did not synchronize well with the treble clef causing unpleasant sound in some part of the music. For the generated music with key E minor and tempo of 120 Bpm, the music generated is horrible which causes by the tempo is too fast. This shows, the LSTM music able to produce pleasent music in slow tempo compared to high tempo. Among three generated music, the best music produced by LSTM is the music with key of C major and tempo of 90 Bpm.   

### VAE

#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/3dda8ed3-a5dd-435d-aef1-2acf5a00c88d" width=400 />
</div>
<p align="middle">
    <em>Music Generated by VAE (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">
   
<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/39bded7e-b94f-4f3c-b8c1-55d201f109b5" width=400 />
</div>
<p align="middle">
    <em>Music Generated by VAE (Key: A Minor, Tempo: 60 Bpm)</em>
</p>

#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/708d608c-6c88-48f5-9a12-f3fe120a6481" width=400 />
</div>
<p align="middle">
    <em>Music Generated by VAE (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on VAE

From all the generated music, the main difference between generated music by VAE and generated music by LSTM is the duration of the music itself. The duration of the music generated by VAE is longer compared to the music generated by LSTM. All the music generated by VAE have almost similar sounds which indicate the low diversity of the music generated. Among the generated musics, the generated music with key of C major and tempo of 90 have shown some characteristic of a good music but the music element seems to become more random after a few seconds of the music start. Overall, The VAE produced little to no structure of music elements which increase the disorderness of music element in the generated music. 

### GAN
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/e0108611-4392-4ab3-a484-323af55c5cd5" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GAN (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/f7de9ebf-cad7-4114-bdff-968252e60ba0" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GAN (Key: A Minor, Tempo: 60 Bpm)</em>
</p>


#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/06d8ce0c-65e2-4c90-94e3-9e5b00bb2b5d" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GAN (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on GAN

Similar with music generated by LSTM, the music generated by GAN have longer duration comapared to LSTM. All the music generated by GAN have almost similar sounds which indicates the low diversity of the music generated. Overall, The VAE produced little to no structure of music elements which increase the disorderness of music element in the generated music and also the music generated by GAN have no melody at all. 

### RNN-VAE-GAN
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/16b72497-460f-4a3a-8d93-0a078455912a" width=400 />
</div>
<p align="middle">
    <em>Music Generated by RNN-VAE-GAN (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/73a5fc07-bb83-4238-ac54-6a19d8de7148" width=400 />
</div>
<p align="middle">
    <em>Music Generated by RNN-VAE-GAN (Key: A Minor, Tempo: 60 Bpm)</em>
</p>

#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/9cb70bf0-e8b5-4c4a-9738-bf034179391a" width=400 />
</div>
<p align="middle">
    <em>Music Generated by RNN-VAE-GAN (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on RNN-VAE-GAN

In the music generated by RNN-VAE-GAN, in all pieces of music generated, the treble clef has more notes compared to the bass clef but they did not compliment very well. The  All the notes generated also are very random producing a noise rather than music. This indicates a little and almost no structure of music elements which increase the disorderness of music element in the generated music. In the sense of melody, the music generated has little melody to the music causing to produce unpleasant sounds.

### GPT
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/a2243b94-56cc-450a-bc58-65afc8bafe13" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GPT (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/ce2d57d5-715e-48c8-ab0e-fec132b66d42" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GPT (Key: A Minor, Tempo: 60 Bpm)</em>
</p>

#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/a068d86b-a159-432f-a7f3-29171c60f48b" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GPT (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on GPT

Based on the three generated music by GPT. GPT has shown high degree of ability to produce pleasant music. This can be proven from the two pieces of generated music (key C major and tempo 90 Bpm) and (key A minor and tempo 60 Bpm) where the both successfuly create a good melody, well structured and expressive. As can be seen in the music generated by using key of C major and tempo of 90 Bpm where the generated music sounds like a salsa music and the bass clef sounds like latin dance. The better music can be seen through music piece with key of A minor and tempo of 60 Bpm where the music generated is more like a sonatina piece (one of clasical music), the bass didn't overwhelm the treble, the treble is more of a continuous streams of notes playing, resembling the first movement sonata. However, given the sucess of producing a great music, the GPT is seen to produce a terrible music which can be seen in the piece of music with key of E minor and tempo of 120 Bpm. Among three generated music, the best music produced by GPT is the music with key of A Minor and tempo of 60 Bpm.

## Evaluation on All Models

# Conclusion

   
