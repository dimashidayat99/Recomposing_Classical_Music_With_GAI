# Recomposing Classical Music Utilizing Generative AI

![](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/6072d90a-82d6-4aae-80af-deca88fab090)


# Background of the Study

Music is a universal language that transcends cultural boundaries and speaks to the depths of human emotion and experience. Classical music is one of the most enduring and beloved forms of music. Rooted in centuries of tradition and innovation, classical music encompasses a vast repertoire of compositions that have stood the test of time.  Classical music, with its rich history and diverse repertoire, has long been a source of inspiration and admiration for musicians and audiences alike. For its complexity, beauty, and emotional depth, the compositions of great classical composers such as Bach, Beethoven, Mozart, and others are revered.

Even though classical music is not popular today, classical music does not remain static. It's continuing to evolve and be an inspiration for new generations of musicians and composers. The use of technology, particularly artificial intelligence (AI), is one way in which classical music has evolved. AI can change the way music is composed, performed, and listened to, opening up new possibilities for creativity and expression. The growing interest in using AI in recent years has promoted the usage of AI in so many fields including music generation. Generative AI, in particular, has shown promise in this regard. By analyzing vast amounts of musical data, generative AI algorithms can learn the patterns and structures that define a particular style or genre of music.  This allows them to compose new compositions that are stylistically similar to those of the composers they've been trained in. 

There are several possible benefits to the revival of classic music using generative AI. New instruments and techniques for the creation of music may be made available to composers, enabling them to explore new creative possibilities. Introducing classical music to new audiences freshly and engagingly can also help preserve and support classical music. The use of generative artificial intelligence for music composition also presents challenges and considerations that need to be addressed. Ensuring that the compositions created with artificial intelligence are original and creative rather than merely imitating existing works is one of the main challenges. In addition, questions are being raised about the role of a composer in AI's creative process and how to attribute and recognize music generated by artificial intelligence.

# Research Questions

The project will based on the following research questions. These research questions are designed to provide a comprehensive exploration of the use of generative AI for recomposing classical music, covering technical, and artistic aspects of this emerging field.

1. How to create generative AI algorithms to produce music similar to classical music?
2. What kind of generative AI algorithms can produce the most pleasant music and approach the characteristics of classical music?
3. How do AI-generated classical music compositions compare to established works in the classical repertoire in terms of artistic expression, as evaluated by musicians and audiences?

# Research Objectives

The objective of this project is to investigate the use of generative AI for recomposing classical music. Specifically, the study aims to achieve the following objectives:

1. To develop generative AI algorithms to produce music similar to classical music.
2. To evaluate which generative AI algorithms can produce the most pleasant music and approach the characteristics of classical music.
3. To compare the AI-generated classical music compositions with the established work in the classical repertoire in terms of artistic expression which is evaluated by musicians and audiences.

# Methodology

## Dataset
The dataset used in this project was obtained from [kaggle](https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi). Where the dataset consists of classical piano midi files containing compositions of 19 famous composers scraped from [piano-midi website](http://www.piano-midi.de). From all famous composers, this project utilizes classical music from only one composer which is known as Chopin. The reason why only one composer was used is due to resource limitation, cost, and time efficiency.

## EDA 
<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/framework/eda_framework.png"  width="300" />
</p>
<p align="middle">
    <em>Exploratory Data Analysis Workflow.</em>
</p>

The process of Exploratory Data Analysis (EDA) is shown in the figure above where it begins with gathering the classical music midi data. The data underwent a sampling process where only Chopin's music will be chosen. There are two types of EDA, the first type of EDA is directly toward the exploration of the music composition itself without data preprocessing where the music sheet and audio are produced to get a better understanding of the music on how music sounds and how the music notes and durations looks like.

## Music Composition
<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/music_sheet_audio/eda_sheet_music.png"  width="800" />
</p>
<p align="middle">
    <em>Music Sheet of One of the Chopin Composed Music.</em>
</p>


<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/e531ea2b-ec2f-4fc1-b451-e12e0e71d873" width=400 />
</div>
<p align="middle">
    <em>Music Audio of One of the Chopin Composed Music</em>
</p>

The process continued with the data preprocessing where each of the Chopin-composed music underwent clef splitting where the treble and bass clef were separated. Each of the clefs was undergone the music element extraction where the notes, chords, and durations were extracted for the data frame construction. There are two total data frames were created which are the treble data frame and bass data frame, each representing the treble clef and bass clef respectively. Each data frame consists of two features where the first feature is the notes and chords while the second feature is durations. The chords and notes were extracted based on their pitch frequency while the durations were extracted based on the durations type. Each of the data frames underwent the second type of EDA which is more toward data analytics. The EDA includes univariate analysis, multivariate analysis, and outliers analysis. 

## Univariate Analysis
### Bass Clef
#### Pitch Frequency Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/pitch_frequency_distribution_bass.png"  width="800" />
</p>
<p align="middle">
    <em>Pitch Frequency Distribution for Bass Clef Data.</em>
</p>

The distribution of pitch frequency is positively skewed with the modes around 160 $s^-{1}$. The majority of the chords and notes used are in the frequencies of 50 $s^{-1}$ to 300 $s^{-1}$. While chords and notes with frequencies around 500 $s^{-1}$ and above can be seen as minority chords and notes used in the Chopin-composed music. 

#### Duration Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/durations_distribution_bass.png"  width="800" />
</p>
<p align="middle">
    <em>Duration Distribution for Bass Clef Data.</em>
</p>

The majority of the durations used are 8th notes with around 17500 counts followed by 16th notes with around 14000 counts. The quarter notes are in the third position with around 7500 counts. Other than the mentioned durations which are whole note, half note, complex note, breve note, and 32nd note are the minority durations in the Chopin-composed music.

### Treble Clef
#### Pitch Frequency Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/pitch_frequency_distribution_treble.png"  width="800" />
</p>
<p align="middle">
    <em>Pitch Frequency Distribution for Treble Clef Data.</em>
</p>

The distribution of pitch frequency is positively skewed with the modes around 350 $s^-{1}$. The majority of the chords and notes used are in the frequencies of 200 $s^{-1}$ to 1300 $s^{-1}$. While chords and notes with frequencies around 1500 $s^{-1}$ and above can be seen as minority chords and notes used in the Chopin-composed music.

#### Duration Distribution

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/univariate_analysis/durations_distribution_treble.png"  width="800" />
</p>
<p align="middle">
    <em>Duration Distribution for Treble Clef Data.</em>
</p>

The majority of the duration used is 16th notes followed by 8th note. The quarter note is in the third position with around 6500 counts. Other than the mentioned durations which are whole note, half note, complex note, breve note, and 32nd note are the minority durations in Chopin composed music.

## Multivariate Analysis
### Bass Clef

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/multivariate_analysis/durations_vs_pitch_frequency_bass.png"  width="800" />
</p>
<p align="middle">
    <em>Violnin Plot of Duration against Pitch Frequency for Bass Clef Data.</em>
</p>


The 8th note, 16th note, and quarter note have almost similar shapes of distribution which is almost symmetric. Majority of the notes are in the range of 30 $s^{-1}$ to 400 $s^{-1}$. The range of 8th note distribution is around 20 $s^{-1}$ to 1300 $s^{-1}$ while te range of 16th note distribution is around 20 $s^{-1}$ to 1400 $s^{-1}$. The half note has a similar distribution to the majority of the notes but is positively skewed with the mode around 100 $s^{-1}$. The whole note has a mode around 80 $s^{-1}$ with the majority of the notes in the range of 20 $s^{-1}$ to 200 $s^{-1}$. The complex note has the range of distribution of 0 $s^{-1}$ to 1400 $s^{-1}$ with the majority of the notes in the range of 0 $s^{-1}$ to 400 $s^{-1}$. Both the breve note and 32nd note have almost the same shape of distribution with almost the same modes around 100 $s^{-1}$. Also, both have the same range of distribution which from 0 $s^{-1}$ to around 360 $s^{-1}$.

### Treble Clef

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/eda/multivariate_analysis/durations_vs_pitch_frequency_treble.png"  width="800" />
</p>
<p align="middle">
    <em>Violnin Plot of Duration against Pitch Frequency for Treble Clef Data.</em>
</p>

The half note, quarter note, 8th note and 16th note have almost similar shape of distribution which is slight positively skewed with almost similar range where most notes are in the range of 200 $s^{-1}$ to 1000 $s^{-1}$ with the distribution range of 200 $s^{-1}$ to 2900 $s^{-1}$. The whole note has a mode around 450 $s^{-1}$ with distribution range of 150 $s^{-1}$ to 1000 $s^{-1}$. The complex note has the longest range of distribution of 0 $s^{-1}$ to 3000 $s^{-1}$. The breve note distribution is the only negatively skewed in the treble clef dsitribution, the breve note distribution has mode around 500 $s^{-1}$ with the range of 200 $s^{-1}$ to 800 $s^{-1}$. The 32nd note distribution has mode around 500 $s^{-1}$ with the range of 0 $s^{-1}$ to 1750 $s^{-1}$ with the majority of the notes lies in the range of 200 $s^{-1}$ to 800 $s^{-1}$. 

The additional and details of the EDA process can be found in `eda` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/eda)


## Modelling

There are so many generative models available which can be used in this project. However, among all the generative models, only 4 models will be used which are the autoregressive model known as Recurrent Neural Network (RNN) by using Long Short-Term Memory (LSTM) architecture, Variational AutoEncoder (VAE), Generative Adversarial Network (GAN) and large language model of Generative Pretrained Transformer (GPT). Another model used in this project is a fusion model which is a combination of three different generative model architecture. The fusion model utilizing the RNN of Gate Recurrent Unit (GRU) architecture combines VAE architecture together with GAN architecture. Each of the models requires a different representation of the data, therefore, different preprocessing was applied to the data for each model. 

### Autoregressive Model - LSTM

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/lstm/framework/LSTM_framework.png" />
</p>
<p align="middle">
    <em>LSTM Model Framework.</em>
</p>

LSTM is a type of neural network designed to handle sequence data by maintaining long-term dependencies. It does this by using a memory cell that can store information, along with gates that control the flow of information into and out of the cell. This allows LSTM networks to learn patterns in sequences and make predictions based on them, making them useful for the main task in this project which is classical music recomposition. A detailed explanation of the LSTM framework as well as the source code can be found in the `model\lstm` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/lstm).

### AutoEncoder Model - VAE

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/vae/framework/VAE_framework.png" />
</p>
<p align="middle">
    <em>VAE Model Framework.</em>
</p>


VAE is a type of neural network used for generative modeling. It learns to encode input data into a lower-dimensional latent space and then decode it back to the original input. VAEs are trained to maximize the likelihood of the observed data while also maximizing the mutual information between the latent variables and the data. This allows them to generate new data samples that resemble the training data, making them useful for the main task in this project which is classical music recomposition.  A detailed explanation of the VAE framework as well as the source code can be found in the `model\vae` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/vae).

### Generative Adversarial Network - GAN

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/gan/framework/GAN_framework.png" />
</p>
<p align="middle">
    <em>GAN Model Framework.</em>
</p>


GAN is a type of neural network used for generative modeling. It consists of two networks: a generator and a discriminator. The generator creates new data samples, while the discriminator tries to distinguish between real and generated samples. They are trained together in an adversarial manner, where the generator tries to fool the discriminator and the discriminator tries to become better at distinguishing real from fake. This competition leads to the generator learning to create increasingly realistic samples, making GANs useful for the main task in this project which is classical music recomposition. A detailed explanation of the GAN framework as well as the source code can be found in the `model\gan` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/gan).

### Fusion Model - RNN-VAE-GAN

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/fusion/framework/FUS_framework.png" />
</p>
<p align="middle">
    <em>Fusion Model Framework.</em>
</p>

A fusion generative model that combines GRU, VAE, and GAN elements would be a complex architecture aimed at capturing the strengths of each component. The GRU component will be used for sequence modeling. GRUs are a type of RNN that can capture long-range dependencies in sequences, making them suitable for tasks where the context of previous elements is important. While the VAE component would handle the latent space modeling. It would encode input data into a latent space representation, which can then be decoded back into the original data. VAEs are useful for learning meaningful representations of data and generating new samples that resemble the training data. Finally, the GAN component would add a competitive training mechanism to improve the realism of the generated samples. The generator part of the GAN would likely be integrated with the VAE decoder, generating samples from the learned latent space. The discriminator would then provide feedback to both the generator and the VAE on the realism of the generated samples.

In this fusion model, the GRU would help capture the sequential dependencies in the data, the VAE would learn a meaningful latent space representation, and the GAN would improve the quality of the generated samples. By combining these components, the model could potentially achieve better performance than using any one component alone. A detailed explanation of the fusion model framework as well as the source code can be found in the `model\fusion` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/fusion).

### Large Language Model - GPT

<p align="middle">
<img src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/gpt/framework/GPT_framework.png" />
</p>
<p align="middle">
    <em>GPT Model Framework.</em>
</p>

GPT is a type of deep learning model based on the transformer architecture. It is designed for natural language processing tasks such as text generation, translation, and question answering. However, since text generation is a sequence data and the music also is a sequence data. The GPT should be able to model music data, in a way that captures the structure and patterns of music. GPT is "pre-trained" on a large corpus of musical data, which allows it to learn the nuances of music elements and context. This pre-training is followed by fine-tuning specific tasks to further improve performance. A detailed explanation of the GPT framework as well as the source code can be found in the `model\gpt` [directory](https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/tree/8a0cc61387d9b473845fe8fda830214b0ab490fa/model/gpt).

# Result & Evaluation

There are three pieces of music generated for each model. The first music was generated by using the key of C major and a tempo of 90 bpm. The second music was generated by using the key of A minor and a tempo of 60 bpm. Finally, the final music was generated by using the key of E minor and a tempo of 120 bpm. All the music generated uses a time signature of 4/4 which is the most common time signature. All the pieces of music was evaluated by the audience and subject matter expert (musician).

## Results
### LSTM
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/cdc40b47-6368-4b38-bd2b-00e24c1be49c" width=400 />
</div>
<p align="middle">
    <em>Music Generated by LSTM (Key: C Major, Tempo: 90 Bpm)</em>
</p>


#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/109e6fa7-cc9b-40cf-9c01-592e70b09a2f" width=400 />
</div>
<p align="middle">
    <em>Music Generated by LSTM (Key: A Minor, Tempo: 60 Bpm)</em>
</p>


#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/4aab4dd9-0dce-488b-8631-68bf5486cdb3" width=400 />
</div>
<p align="middle">
    <em>Music Generated by LSTM (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on LSTM

Based on the three generated music by LSTM. LSTM has shown some degree of ability to produce pleasant music. This can be proven from the two pieces of generated music (key C major and tempo 90 Bpm) and (key A minor and tempo 60 Bpm) where they both successfully create a good melody and are well-structured in the treble clef. However, the bass clef generated did not synchronize well with the treble clef causing unpleasant sounds in some parts of the music. For the generated music with key E minor and a tempo of 120 Bpm, the music generated is horrible which causes the tempo to be too fast. This shows that LSTM music is able to produce pleasant music in a slow tempo compared to a high tempo. Among the three generated music, the best music produced by LSTM is the music with the key of C major and tempo of 90 Bpm.   

### VAE

#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/3dda8ed3-a5dd-435d-aef1-2acf5a00c88d" width=400 />
</div>
<p align="middle">
    <em>Music Generated by VAE (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">
   
<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/39bded7e-b94f-4f3c-b8c1-55d201f109b5" width=400 />
</div>
<p align="middle">
    <em>Music Generated by VAE (Key: A Minor, Tempo: 60 Bpm)</em>
</p>

#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/708d608c-6c88-48f5-9a12-f3fe120a6481" width=400 />
</div>
<p align="middle">
    <em>Music Generated by VAE (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on VAE

From all the generated music, the main difference between generated music by VAE and generated music by LSTM is the duration of the music itself. The duration of the music generated by VAE is longer compared to the music generated by LSTM. All the music generated by VAE have almost similar sounds which indicate the diversity compared to training data. However, due to the diversity of music generated compared to training data, the music element generated is produced very randomly causing little to no structure of music elements which increases the disorderness of music elements in the generated music. Among the generated music, the generated music with the key of C major and tempo of 90 has shown some characteristics of good music but the music element seems to become more random after a few seconds of the music start.

### GAN
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/e0108611-4392-4ab3-a484-323af55c5cd5" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GAN (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/f7de9ebf-cad7-4114-bdff-968252e60ba0" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GAN (Key: A Minor, Tempo: 60 Bpm)</em>
</p>


#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/06d8ce0c-65e2-4c90-94e3-9e5b00bb2b5d" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GAN (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on GAN

Similar to music generated by LSTM, the music generated by GAN has a longer duration compared to LSTM. All the music generated by GAN has almost similar sounds which indicates the low diversity of the music generated. Overall, The VAE produced little to no structure of music elements which increases the disorderness of music elements in the generated music, and also the music generated by GAN has no melody at all. 

### RNN-VAE-GAN
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/16b72497-460f-4a3a-8d93-0a078455912a" width=400 />
</div>
<p align="middle">
    <em>Music Generated by RNN-VAE-GAN (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/73a5fc07-bb83-4238-ac54-6a19d8de7148" width=400 />
</div>
<p align="middle">
    <em>Music Generated by RNN-VAE-GAN (Key: A Minor, Tempo: 60 Bpm)</em>
</p>

#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/9cb70bf0-e8b5-4c4a-9738-bf034179391a" width=400 />
</div>
<p align="middle">
    <em>Music Generated by RNN-VAE-GAN (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on RNN-VAE-GAN

In the music generated by RNN-VAE-GAN, in all pieces of music generated, the treble clef has more notes compared to the bass clef but they did not complement very well. All the notes generated also are very random producing a noise rather than music. This indicates little and almost no structure of music elements which increases the disorderness of music elements in the generated music. In the sense of melody, the music generated has little melody to the music causing it to produce unpleasant sounds.

### GPT
#### Key: C Major, Tempo: 90 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/a2243b94-56cc-450a-bc58-65afc8bafe13" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GPT (Key: C Major, Tempo: 90 Bpm)</em>
</p>

#### Key: A Minor, Tempo: 60 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/ce2d57d5-715e-48c8-ab0e-fec132b66d42" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GPT (Key: A Minor, Tempo: 60 Bpm)</em>
</p>

#### Key: E Minor, Tempo: 120 Bpm
<div align="center">

<video src="https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/assets/69446089/a068d86b-a159-432f-a7f3-29171c60f48b" width=400 />
</div>
<p align="middle">
    <em>Music Generated by GPT (Key: E Minor, Tempo: 120 Bpm)</em>
</p>

#### Evaluation on GPT

Based on the three generated music by GPT. GPT has shown a high degree of ability to produce pleasant music. This can be proven from the two pieces of generated music (key C major and tempo 90 Bpm) and (key A minor and tempo 60 Bpm) where they both successfully create a good melody, well structured and expressive. As can be seen in the music generated by using the key of C major and tempo of 90 Bpm where the generated music sounds like salsa music and the bass clef sounds like Latin dance. The better music can be seen through the music piece with the key of A minor and tempo of 60 Bpm where the music generated is more like a sonatina piece (one of classical music), the bass didn't overwhelm the treble, the treble is more of a continuous stream of notes playing, resembling the first movement sonata. However, given the success of producing great music, the GPT is seen to produce terrible music which can be seen in the piece of music with key of E minor and tempo of 120 Bpm. This indicates that the GPT model is able to produce great music at low tempo compared to high tempo. Among the three generated music, the best music produced by GPT is the music with the key of A Minor and a tempo of 60 Bpm.

## Evaluation on All Models

The LSTM model, while capable of generating a structured music sequence, often lacked of coherent music sequence in terms of synchronization of treble and bass clef. The VAE model, on the other hand, was successful in generating diverse compositions but struggled with maintaining the musical structure which caused a disorderliness and randomness of music elements generated. The GAN model also has a similar limitation to the VAE which is struggled with maintaining music structure. The RNN-VAE-GAN fusion model attempted to combine the strengths of the VAE and GAN models with an additional RNN layer for improved musical structure. While it showed improvements over individual models, it still faced challenges in balancing coherence, diversity, and maintaining musical structure. Despite not being trained on a large corpus of language, the GPT model outperformed all other models in our evaluation. It is able to generate highly coherent, diverse, musical structures and musically pleasing compositions. Its transformer architecture demonstrated a remarkable ability to capture the nuances of classical music, producing compositions that were indistinguishable from human-composed pieces.

## Establish Classical Music VS AI generated Music

Established classical music, crafted over centuries by master composers, embodies a profound legacy of artistic expression and cultural significance, characterized by intricate melodies and harmonies that resonate with deep emotional and intellectual depth. In contrast, AI-generated music represents a novel frontier, leveraging machine learning and algorithms to produce compositions that, while lacking the human touch and historical context of classical masterpieces, offer a fresh perspective and potential for innovation. While AI-generated music may not yet match the depth and complexity of classical compositions, it presents a fascinating evolution of musical creativity, sparking debates about the nature of artistry, the impact of technology on music, and the boundaries of musical expression.

## Limitation

1. Algorithm Complexity: The development of generative AI algorithms for music composition can be complex, requiring sophisticated models and significant computational resources. This complexity may limit the scalability and accessibility of the approach.

2. Subjectivity of Evaluation: The evaluation of music, particularly in terms of its pleasantness and resemblance to classical music, is subjective and can vary among individuals. This subjectivity may introduce bias into the assessment process.

3. Limited Computational Resources: The development and training of generative AI algorithms for music composition require significant computational resources impacting the feasibility and scalability of the approach. This limitation results in the use of the small amount of data and a small number of epochs for training, which can affect the quality and diversity of the generated compositions.

# Conclusion & Future Works

In conclusion, this study has explored the use of generative AI for recomposing classical music, focusing on developing algorithms that can produce music similar to classical compositions. Through the evaluation of various generative AI algorithms, I have identified those capable of creating music that is not only pleasant but also approaches the characteristics of classical music especially music generated by GPT. Additionally, by comparing AI-generated classical music compositions with established works in the classical repertoire, I have gained insights into the artistic expression of these compositions, as evaluated by both musicians and audiences. This study highlights the potential of generative AI in music composition, offering new avenues for creative exploration and pushing the boundaries of musical expression.

For future work, expanding the dataset and training models with more computational resources could enhance the quality and diversity of AI-generated compositions. Exploring ways to incorporate user feedback and preferences into the AI algorithms could also lead to more personalized and engaging musical experiences. Furthermore, investigating the integration of AI-generated music into collaborative environments with human musicians could open up new possibilities for innovative music creation.
