# Autoregressive Model - LSTM

## LSTM Framework

<p align="middle">
<img src=https://github.com/dimashidayat99/Recomposing_Classical_Music_With_GAI/blob/main/model/lstm/framework/LSTM_framework.png/>
</p>
<p align="middle">
    <em>Exploratory LSTM Framework.</em>
</p>

## Preprocessing 
As shown in the framework figure above, the preprocessing start with sampling process where only five midi file was used and only come from Chopin composed music. All the midi file underwent the key tranposing process where all the key of the songs will be transpose to middle C. The midi files was processed further by extracting their parts. Each part was looked for their clef property where parts with treble clef property belongs to treble clef while parts with bass clef property belongs to bass clef. The music element such as notes, chords and durations were extracted for each clef. The notes and chords were grouped together under the same feature while durations itself become another feature. Both features were used to construct a dataframe. There are total 2 dataframe which represent treble and bass clef. Both dataframes were processed further for data transformation, where list of chords were changed to chords label while numerical representation of durations data were transformed to categorical data. Noted that, in music, the duration is mostly represent as categorical representative instead of numerical representative. After transormation, both dataframes were encoded by using label encoding technique. Finally, the sequence data of each clef was produced by extracting 100 sequence data from the dataframe. The final product of the preprocessing stage is two data consist of 100 sequence of notes and durations

## Modelling 

This project used typically one model for each clef. Therefore, there are total two identical models architecture were used for modelling stage. The architecture of LSTM model begins with input Layer. This layer receives the input data, which is the sequence of processed music elements. It does not perform any computations but passes the input to the next layer. The input layer was connected to LSTM layer which is capable of learning long-term dependencies. In this layer, the LSTM units process the input sequences and learn to capture patterns in the data over time. A dropout was applied to the architecture sequence. A dropout is a regularization technique used to prevent overfitting. It randomly sets a fraction of input units to zero during training, which helps to reduce the model's reliance on specific input features. Another LSTM layer was added after the first dropout layer to further capture complex patterns in the data. Another dropout layer was added after the second LSTM layer to regularize the model further. The third LSTM layer was used continues to capture higher-level patterns in the data. Then dense layer was connected to the achitecture sequence. This dense layer is fully connected, meaning each neuron in this layer is connected to every neuron in the previous layer. It helps in learning complex patterns from the LSTM layers' outputs. The dense layer output was split into two separate paths where the first branch was connected to the dense layer with softmax activation function. The softmax activation function was used to transforms the raw outputs of the neural network into a vector of probabilities. This dense layer for the first branch, which produces the output for the notes. The second branch was also connected to another dense layer with softmax activation function where this layer for the second branch, which produces the output for the durations. The loss function used in the LSTM model is sparse categorical crossentropy. The final product of the models will be the generated notes and durations in the form of probability across all notes used and durations used in the dataset.

## Music Generation
The outputs (notes and durations) were underwent the top k sampling process where one from top ten notes and one from top five durations will be randomly select. The dataframe was constructed based on the select notes and durations. Since there two clef, there are two total dataframe which are treble clef dataframe and bass clef dataframe. The dataframes will undergo the inverse label encoding and inverse transformation where the numerical representation of data were transformed to categorical representation of data. The processed generated dataframes were submitted to the music stream, where the generated treble data was submitted to the treble clef stream while the generated bass data was submitted to the bass clef stream. Both streams went through the stream balancing process where the both streams length were adjusted to make sure both length are similar (or almost similar). The balanced streams were submitted to the music score. The music score was used to genete the music in the form of music audio and music sheet.

